{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "flair_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Import and installations"
      ],
      "metadata": {
        "id": "qhuBvwa0mMEf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7i81Yw3jgl9T"
      },
      "source": [
        "import numpy as np\n",
        "import bz2\n",
        "import json\n",
        "import os.path\n",
        "import pandas as pd\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNJQIjxAhdEB",
        "outputId": "94b1b2eb-dc82-4577-84cb-4c6366512c3b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtUYZvVwg3o8"
      },
      "source": [
        "!pip install tld\n",
        "!pip install flair\n",
        "!pip install pyarrow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rl3Xq_xVg2cC"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "e8p7mNaasGaR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO : change \"/content/drive/MyDrive/Colab Notebooks/ada/\"\n",
        "path_files = \"/content/drive/MyDrive/Colab Notebooks/ada/\"\n",
        "\n",
        "path_us = path_files + \"us_data/Filtered data/\""
      ],
      "metadata": {
        "id": "HJ57mQianJqS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tools"
      ],
      "metadata": {
        "id": "nktVmqTRl1_R"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xx0s4Y25gl9m"
      },
      "source": [
        "#Flair model using BERT\n",
        "def sia_flair_1():\n",
        "    start_time = time.time()\n",
        "    print(\"Loading flair model...\")\n",
        "    sia = TextClassifier.load('en-sentiment')\n",
        "    print(\"Loading model done in  %s seconds.\" % (time.time() - start_time))\n",
        "    return sia\n",
        "\n",
        "def pred_flair_1(x, sia, batch_size):\n",
        "    x_np = x.to_numpy()\n",
        "    sentences = list(np.vectorize(lambda a : Sentence(a))(x_np))\n",
        "    sia.predict(sentences,return_probabilities_for_all_classes=True, mini_batch_size=batch_size)\n",
        "    scores = [ -1 * sent.labels[0].score + sent.labels[1].score for sent in sentences]\n",
        "    return np.array(scores)\n",
        "\n",
        "#Flair model using RNN : Faster\n",
        "def sia_flair_2():\n",
        "    start_time = time.time()\n",
        "    print(\"Loading flair model...\")\n",
        "    sia = TextClassifier.load('sentiment-fast')\n",
        "    print(\"Loading model done in  %s seconds.\" % (time.time() - start_time))\n",
        "    return sia\n",
        "\n",
        "def pred_flair_2(x, sia, batch_size):\n",
        "    x_np = x.to_numpy()\n",
        "    sentences = list(np.vectorize(lambda a : Sentence(a))(x_np))\n",
        "    sia.predict(sentences,return_probabilities_for_all_classes=True, mini_batch_size=batch_size)\n",
        "    scores = [ -1 * sent.labels[1].score + sent.labels[0].score for sent in sentences]\n",
        "\n",
        "    return np.array(scores)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "501TiMmapK-A"
      },
      "source": [
        "'''\n",
        "print accuracy given predictions and the test set\n",
        "'''\n",
        "def accuracy(test, output):\n",
        "    res = (test == output).sum()\n",
        "    print(\"accuracy : \" + str(res / len(test)) + \" %\")\n",
        "    return res / len(test)\n",
        "\n",
        "'''\n",
        "print the mean square error given the prediction and the test set\n",
        "'''\n",
        "def mse(test, predictions):\n",
        "    res = ((test - predictions)**2).sum() / len(test)\n",
        "    print('mse : ' + str(res))\n",
        "    return res\n",
        "\n",
        "'''\n",
        "given predictions between [-1, 1], return labels {-1, 0, 1}\n",
        "uniform : if the range is equally splitted : \n",
        "-1 if [-1, -1/3]\n",
        "0 if [-1/3, 1/3]\n",
        "-1 if [1/3, 1]\n",
        "'''\n",
        "def classify(predictions, uniform=False):\n",
        "    if uniform :\n",
        "        return predictions.vectorize(lambda x: -1 if x < -1/3 else 0 if x < 1/3 else 1)   \n",
        "    else :\n",
        "        return np.round(predictions)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess"
      ],
      "metadata": {
        "id": "ouwaAkcImG-C"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3LOLQCfgl9n"
      },
      "source": [
        "#Load test sample manually labelled\n",
        "test_sample = pd.read_csv(path_us + \"sample_quotes_labelled.csv\")  "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KE-xfjkDgl9n",
        "outputId": "26312e70-e774-4915-96ba-777816c0dc7a"
      },
      "source": [
        "#Loading flair model BERT\n",
        "sia1 = sia_flair_1()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading flair model...\n",
            "2021-12-13 09:24:04,482 https://nlp.informatik.hu-berlin.de/resources/models/sentiment-curated-distilbert/sentiment-en-mix-distillbert_4.pt not found in cache, downloading to /tmp/tmptaz8uy_m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 265512723/265512723 [00:29<00:00, 8975861.41B/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-13 09:24:34,903 copying /tmp/tmptaz8uy_m to cache at /root/.flair/models/sentiment-en-mix-distillbert_4.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-13 09:24:35,426 removing temp file /tmp/tmptaz8uy_m\n",
            "2021-12-13 09:24:35,464 loading file /root/.flair/models/sentiment-en-mix-distillbert_4.pt\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "08e128e99a784fa4bb7a60299aef93b9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5beaa76f2e0049d2a66d3fbde7c3e3fc",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a4f6e76c2fab4e1886beb512b9b908da",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "26ab86e7ee1545d2b9081756bc6378d4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model done in  56.46393418312073 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSgN-ZukixSc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4f53833-9258-4220-c6a0-cb690eb0fd9f"
      },
      "source": [
        "#Loading flair model RNN\n",
        "sia2 = sia_flair_2()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading flair model...\n",
            "2021-12-13 09:25:00,977 https://nlp.informatik.hu-berlin.de/resources/models/sentiment-curated-fasttext-rnn/sentiment-en-mix-ft-rnn_v8.pt not found in cache, downloading to /tmp/tmplbweqtss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1241977025/1241977025 [02:13<00:00, 9281532.97B/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-13 09:27:15,664 copying /tmp/tmplbweqtss to cache at /root/.flair/models/sentiment-en-mix-ft-rnn_v8.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-13 09:27:20,108 removing temp file /tmp/tmplbweqtss\n",
            "2021-12-13 09:27:20,277 loading file /root/.flair/models/sentiment-en-mix-ft-rnn_v8.pt\n",
            "Loading model done in  150.95562291145325 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracy comparison"
      ],
      "metadata": {
        "id": "0YNyacrvl7D7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models_name = [\"Flair\", \"Flair_fast\"]\n",
        "models_sia = [sia1, sia2]\n",
        "models_pred = [pred_flair_1, pred_flair_2]\n",
        "\n",
        "for i in range(len(models_name)):\n",
        "    print(models_name[i] + ' Analysis')\n",
        "    print(\"===========================\")\n",
        "\n",
        "    pred = models_pred[i]\n",
        "    #predict sentiment foreach quotation in the test set\n",
        "    #-1 : negative, 0 : neutral, 1 positive\n",
        "    predictions = pred(test_sample.quotation, models_sia[i], batch_size=16)\n",
        "    \n",
        "    mse(test_sample[\"sentiment\"], predictions)\n",
        "    \n",
        "    sentiments = classify(predictions)\n",
        "    accuracy(test_sample[\"sentiment\"], sentiments)\n",
        "\n",
        "    print()\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMXVxPwsEtwb",
        "outputId": "e2b64514-ecdb-42e0-a958-939434f76327"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flair Analysis\n",
            "===========================\n",
            "mse : 0.4684054682271506\n",
            "accuracy : 0.66 %\n",
            "\n",
            "Flair_fast Analysis\n",
            "===========================\n",
            "mse : 0.5881031352434072\n",
            "accuracy : 0.56 %\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Runtime comparison"
      ],
      "metadata": {
        "id": "zt-WpEl_E01A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note : running on GPU"
      ],
      "metadata": {
        "id": "2dQUbfHYGWcT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bo2sZ8pQgl9p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6da5965d-346e-4636-c3d8-e5d39955cc19"
      },
      "source": [
        "df_sample = pd.read_pickle(path_us + \"us_2020.pkl.bz2\",compression='bz2')[['quoteID', 'quotation']].sample(10000)\n",
        "len(df_sample)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sizes = [8, 16, 32, 64, 128]\n",
        "for s in sizes: \n",
        "    print(\"mini-batch size : \" + str(s))\n",
        "    start_time = time.time()\n",
        "    predictions = pred_flair_1(df_sample.quotation, sia1, batch_size=s)\n",
        "    \n",
        "    print(\"Runtime flair:  %s seconds \" % (time.time() - start_time))\n",
        "    \n",
        "    start_time = time.time()\n",
        "    predictions = pred_flair_2(df_sample.quotation, sia2, batch_size=s)\n",
        "    print(\"Runtime flair_fast:  %s seconds \" % (time.time() - start_time))\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dw1BFGFaEhtZ",
        "outputId": "126760bc-725b-4583-eaed-7f0c4d134fb7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mini-batch size : 8\n",
            "Runtime flair:  53.888055086135864 seconds \n",
            "Runtime flair_fast:  20.321967124938965 seconds \n",
            "\n",
            "mini-batch size : 16\n",
            "Runtime flair:  48.28710865974426 seconds \n",
            "Runtime flair_fast:  18.333975076675415 seconds \n",
            "\n",
            "mini-batch size : 32\n",
            "Runtime flair:  46.5600368976593 seconds \n",
            "Runtime flair_fast:  15.473497152328491 seconds \n",
            "\n",
            "mini-batch size : 64\n",
            "Runtime flair:  46.0074577331543 seconds \n",
            "Runtime flair_fast:  15.76470422744751 seconds \n",
            "\n",
            "mini-batch size : 128\n",
            "Runtime flair:  49.31395673751831 seconds \n",
            "Runtime flair_fast:  14.79665207862854 seconds \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Discussion"
      ],
      "metadata": {
        "id": "AMJzw6UiJtDQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Despite the good accuracy that the first Flair model gives us, its runtime with the computational power we have make its use not practicle.  \n",
        "For example, the filtered dataset of 2017 has more than 1'000'000 quotes, to process it one should wait around 1 hour 30, and this is just for one year.  \n",
        "For this reason we have chosen the Flair fast model that runs around 4 times faster."
      ],
      "metadata": {
        "id": "8lrxwsHWJwFk"
      }
    }
  ]
}