{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_ind\n",
    "import sys\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load pew dataset\n",
    "pew = pd.read_csv('pew.csv')\n",
    "\n",
    "#List of \"invalid\" answers in pew dataset\n",
    "Nan_keywords = ['Refused', 'Don’t know', 'Don’t know (DO NOT READ)', 'Refused (DO NOT READ)',\n",
    "       '(VOL) Refused', \"(VOL)\\xa0Don't know\"]\n",
    "\n",
    "pew = pew.replace(to_replace=Nan_keywords, value=np.NaN)\n",
    "\n",
    "#map \"sentiment\" to float values\n",
    "fav_dict = {'Somewhat favorable': 0.5, 'Very favorable':1,\n",
    "       'Somewhat unfavorable':-0.5, 'Very unfavorable':-1}\n",
    "\n",
    "pew = pew.replace(fav_dict)\n",
    "\n",
    "country_list_pew = ['Argentina', 'Australia', 'Brazil', 'Canada', 'France', 'Germany', 'Greece', 'Hungary', 'India',\n",
    "                'Indonesia', 'Israel', 'Italy', 'Japan', 'Kenya', 'Lebanon', 'Mexico', 'Netherlands', 'Nigeria',\n",
    "                'Philippines', 'Poland', 'Russia', 'South Africa', 'South Korea', 'Spain', 'Sweden', 'Turkey',\n",
    "                'Ukraine', 'United Kingdom', 'United States']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def drop_none_speaker(df):\n",
    "    #print(\"Year : {} with {} quotes\".format(year, len(df)))\n",
    "    drop_mask = df[df.speaker == \"None\"].index\n",
    "    df_dropped = df.drop(drop_mask)\n",
    "    print(\"{} quotes are removed\".format(len(drop_mask)))\n",
    "    return df_dropped\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for year in range(2015, 2021):\n",
    "    print(year)\n",
    "    df1 = pd.read_pickle('us_{}media.pkl.bz2'.format(year), compression='bz2')\n",
    "    df = pd.concat([df, df1])\n",
    "\n",
    "df = drop_none_speaker(df)\n",
    "\n",
    "US_data = pd.DataFrame()\n",
    "US_data = pd.concat([US_data, df], axis=0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "speaker_df = pd.read_parquet(\"speaker_attributes.parquet\")\n",
    "speaker_df = speaker_df[['id', 'nationality']]\n",
    "\n",
    "US_data[\"speaker_id\"] = US_data.qids.map(lambda x: x[0])\n",
    "\n",
    "#join US_data with the corresponding speakers' attributes\n",
    "US_merged = pd.merge(US_data, speaker_df, left_on=['speaker_id'], right_on=['id'], how='inner')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "\n",
    "#sparkql query to get the country name and their respective quids from wikidata\n",
    "query = \"\"\"#List of present-day countries and capital(s)\n",
    "SELECT DISTINCT ?country ?countryLabel\n",
    "WHERE\n",
    "{\n",
    "  ?country wdt:P31 wd:Q3624078 .\n",
    "\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" }\n",
    "}\n",
    "ORDER BY ?countryLabel\"\"\"\n",
    "\n",
    "'''\n",
    "query internet dataset:\n",
    "endpoint_url (string): url of corresponding dataset\n",
    "query (string): sparksql query\n",
    "'''\n",
    "def get_results(endpoint_url, query):\n",
    "    user_agent = \"WDQS-example Python/%s.%s\" % (sys.version_info[0], sys.version_info[1])\n",
    "    # TODO adjust user agent; see https://w.wiki/CX6\n",
    "    sparql = SPARQLWrapper(endpoint_url, agent=user_agent)\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    return sparql.query().convert()\n",
    "\n",
    "\n",
    "results = get_results(endpoint_url, query)\n",
    "\n",
    "#Build a country dictionnary {quid, country name}\n",
    "country_dict = {}\n",
    "for result in results[\"results\"][\"bindings\"]:\n",
    "    country_id = result[\"country\"]['value'].split('/')[-1]\n",
    "    country_dict[country_id] = result[\"countryLabel\"]['value']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Add nationality of speaker from the country dictionnary\n",
    "US_with_nation = US_merged.explode('nationality')\n",
    "US_with_nation.nationality = US_with_nation.nationality.map(country_dict)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "# predict the sentiment of each quotation in the range [-1, 1]\n",
    "US_with_nation[\"sentiment\"] = US_with_nation.quotation.apply(lambda x: sia.polarity_scores(x)[\"compound\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "US_with_nation[\"year\"] = US_with_nation.date.dt.strftime('%Y')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "country_list = ['Argentina', 'Australia', 'Brazil', 'Canada', 'France', 'Germany', 'Greece', 'Hungary', 'India',\n",
    "                'Indonesia', 'Israel', 'Italy', 'Japan', 'Kenya', 'Lebanon', 'Mexico', 'Kingdom of the Netherlands', 'Nigeria',\n",
    "                'Philippines', 'Poland', 'Russia', 'South Africa', 'South Korea', 'Spain', 'Sweden', 'Turkey',\n",
    "                'Ukraine', 'United Kingdom', 'United States of America']\n",
    "\n",
    "num_bin_quotebank = 50\n",
    "bin_lims_quotebank = np.linspace(-1,1,num_bin_quotebank+1)\n",
    "bin_centers_quotebank = 0.5*(bin_lims_quotebank[:-1]+bin_lims_quotebank[1:])\n",
    "bin_widths_quotebank = bin_lims_quotebank[1:]-bin_lims_quotebank[:-1]\n",
    "\n",
    "num_bin = 4\n",
    "bin_lims = np.linspace(0,1,num_bin+1)\n",
    "bin_centers_obama = [-0.125, 0.5-0.125, 1.0-0.125, 1.5-0.125]\n",
    "bin_centers_trump = [0.125, 0.5+0.125, 1.0+0.125, 1.5+0.125]\n",
    "bin_widths = [0.125, 0.125, 0.125, 0.125]\n",
    "\n",
    "bin_centers_obama = [-0.125+0.0625, 0.5+0.0625-0.125, 1.0+0.0625-0.125, 1.5+0.0625-0.125]\n",
    "bin_centers_trump = [0.125-0.0625, 0.5-0.0625+0.125, 1.0-0.0625+0.125, 1.5-0.0625+0.125]\n",
    "bin_widths = [0.125, 0.125, 0.125, 0.125]\n",
    "\n",
    "for country in country_list:\n",
    "    print(country)\n",
    "    cond_obama_pew = (pew[\"country\"] == country) & ((pew[\"year\"] == 2015) | (pew[\"year\"] == 2016))\n",
    "    cond_trump_pew = (pew[\"country\"] == country) & ((pew[\"year\"] == 2019) | (pew[\"year\"] == 2020))\n",
    "\n",
    "    data_pew = pd.DataFrame({'obama': pew[cond_obama_pew]['fav_us'], 'trump': pew[cond_trump_pew]['fav_us']})\n",
    "\n",
    "    t_statistic_pew = ttest_ind(data_pew['obama'].dropna(), data_pew['trump'].dropna())[0]\n",
    "    p_value_pew = ttest_ind(data_pew['obama'].dropna(), data_pew['trump'].dropna())[1]\n",
    "\n",
    "    mean_us_fav_obama_pew = str(round(data_pew['obama'].mean(), 3))\n",
    "    median_us_fav_obama_pew = str(round(data_pew['obama'].median(), 3))\n",
    "\n",
    "    mean_us_fav_trump_pew = str(round(data_pew['trump'].mean(), 3))\n",
    "    median_us_fav_trump_pew = str(round(data_pew['trump'].median(), 3))\n",
    "\n",
    "    hist_obama_pew, _ = np.histogram(pew[cond_obama_pew]['fav_us'].dropna())\n",
    "    hist_obama_norm_pew = hist_obama_pew / np.sum(hist_obama_pew) * 100\n",
    "\n",
    "    hist_trump_pew, _ = np.histogram(pew[cond_trump_pew]['fav_us'].dropna())\n",
    "    hist_trump_norm_pew = hist_trump_pew / np.sum(hist_trump_pew) * 100\n",
    "\n",
    "    ##################################################################\n",
    "\n",
    "    obama_cond = (US_with_nation[\"nationality\"] == country) & ((US_with_nation[\"year\"] == '2015') | (US_with_nation[\"year\"] == '2016'))\n",
    "    trump_cond = (US_with_nation[\"nationality\"] == country) & ((US_with_nation[\"year\"] == '2019') | (US_with_nation[\"year\"] == '2020'))\n",
    "    obama_sentiment = US_with_nation[obama_cond][\"sentiment\"]\n",
    "    trump_sentiment = US_with_nation[trump_cond][\"sentiment\"]\n",
    "    _, p_value = stats.ttest_ind(obama_sentiment,trump_sentiment,alternative='two-sided')\n",
    "    print(\"Country : {},\\tObama: {:.2f}\\tTrump: {:.2f}\".format(country, obama_sentiment.mean(), trump_sentiment.mean()))\n",
    "    print(\"P value: {}\\n\".format(p_value))\n",
    "\n",
    "    ##################################################################\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(13, 13), dpi=200)\n",
    "\n",
    "    fig.suptitle('US fav seen by ' + country +\n",
    "                 '\\n' + 'p-value pew: ' + str(round(p_value_pew, 4)) +\n",
    "                 '\\n' + 'p-value quotebank: ' + str(round(p_value, 4)))\n",
    "\n",
    "    ax1.bar(bin_centers_obama, [hist_obama_norm_pew[0], hist_obama_norm_pew[2], hist_obama_norm_pew[7], hist_obama_norm_pew[9]],\n",
    "            width=bin_widths, align='center')\n",
    "    ax1.bar(bin_centers_trump, [hist_trump_norm_pew[0], hist_trump_norm_pew[2], hist_trump_norm_pew[7], hist_trump_norm_pew[9]],\n",
    "            width=bin_widths, align='center', alpha=0.5)\n",
    "\n",
    "    ax1.set_xticklabels(['', 'very unpopular', '', 'unpopular', '', 'popular', '', 'very popular'], fontdict=None,\n",
    "                        minor=False, rotation=45)\n",
    "    ax1.set_ylabel('%')\n",
    "    ax1.set_title('pew' + '\\n' +\n",
    "                  'mean: ' + mean_us_fav_obama_pew + '  median: ' + median_us_fav_obama_pew + '\\n' +\n",
    "                  'mean: ' + mean_us_fav_trump_pew + '  median: ' + median_us_fav_trump_pew)\n",
    "\n",
    "    hist1, _ = np.histogram(obama_sentiment, bins=bin_lims_quotebank)\n",
    "    hist2, _ = np.histogram(trump_sentiment, bins=bin_lims_quotebank)\n",
    "    hist1b = hist1 / np.sum(hist1)\n",
    "    hist2b = hist2 / np.sum(hist2)\n",
    "\n",
    "    ax2.bar(bin_centers_quotebank, hist1b*100, width=bin_widths_quotebank, align='center')\n",
    "    ax2.bar(bin_centers_quotebank, hist2b*100, width=bin_widths_quotebank, align='center', alpha=0.5)\n",
    "\n",
    "    ax2.set_ylabel('%')\n",
    "    ax2.set_title('quotebank' + '\\n' +\n",
    "                  'mean: ' + str(round(obama_sentiment.mean(), 3)) + '  median: ' + str(round(obama_sentiment.median(), 3)) + '\\n' +\n",
    "                  'mean: ' + str(round(trump_sentiment.mean(), 3)) + '  median: ' + str(round(trump_sentiment.median(), 3)))\n",
    "\n",
    "    plt.savefig('bar plot ' + country)\n",
    "    plt.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}