{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhuBvwa0mMEf"
   },
   "source": [
    "### Import and installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7i81Yw3jgl9T"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import bz2\n",
    "import json\n",
    "import os.path\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eNJQIjxAhdEB",
    "outputId": "94b1b2eb-dc82-4577-84cb-4c6366512c3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RtUYZvVwg3o8"
   },
   "outputs": [],
   "source": [
    "!pip install tld\n",
    "!pip install flair\n",
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "HJ57mQianJqS"
   },
   "outputs": [],
   "source": [
    "#PAth to files\n",
    "path_files = \"/content/drive/MyDrive/Colab Notebooks/ada/\"\n",
    "path_us = path_files + \"us_data/Filtered data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nktVmqTRl1_R"
   },
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "xx0s4Y25gl9m"
   },
   "outputs": [],
   "source": [
    "#Flair model using BERT, loading model function\n",
    "def sia_flair_1():\n",
    "    start_time = time.time()\n",
    "    print(\"Loading flair model...\")\n",
    "    sia = TextClassifier.load('en-sentiment')\n",
    "    print(\"Loading model done in  %s seconds.\" % (time.time() - start_time))\n",
    "    return sia\n",
    "\n",
    "#prediction function of flair that uses BERT\n",
    "def pred_flair_1(x, sia, batch_size):\n",
    "    x_np = x.to_numpy()\n",
    "    sentences = list(np.vectorize(lambda a : Sentence(a))(x_np))\n",
    "    sia.predict(sentences,return_probabilities_for_all_classes=True, mini_batch_size=batch_size)\n",
    "    scores = [ -1 * sent.labels[0].score + sent.labels[1].score for sent in sentences]\n",
    "    return np.array(scores)\n",
    "\n",
    "#Flair model using RNN : Faster, loading model function\n",
    "def sia_flair_2():\n",
    "    start_time = time.time()\n",
    "    print(\"Loading flair model...\")\n",
    "    sia = TextClassifier.load('sentiment-fast')\n",
    "    print(\"Loading model done in  %s seconds.\" % (time.time() - start_time))\n",
    "    return sia\n",
    "\n",
    "#prediction function of flair that uses RNN\n",
    "def pred_flair_2(x, sia, batch_size):\n",
    "    x_np = x.to_numpy()\n",
    "    sentences = list(np.vectorize(lambda a : Sentence(a))(x_np))\n",
    "    sia.predict(sentences,return_probabilities_for_all_classes=True, mini_batch_size=batch_size)\n",
    "    scores = [ -1 * sent.labels[1].score + sent.labels[0].score for sent in sentences]\n",
    "\n",
    "    return np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "501TiMmapK-A"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "print accuracy given predictions and the test set\n",
    "'''\n",
    "def accuracy(test, output):\n",
    "    res = (test == output).sum()\n",
    "    print(\"accuracy : \" + str(res / len(test)) + \" %\")\n",
    "    return res / len(test)\n",
    "\n",
    "'''\n",
    "print the mean square error given the prediction and the test set\n",
    "'''\n",
    "def mse(test, predictions):\n",
    "    res = ((test - predictions)**2).sum() / len(test)\n",
    "    print('mse : ' + str(res))\n",
    "    return res\n",
    "\n",
    "'''\n",
    "given predictions between [-1, 1], return labels {-1, 0, 1}\n",
    "uniform : if the range is equally splitted : \n",
    "-1 if [-1, -1/3]\n",
    "0 if [-1/3, 1/3]\n",
    "-1 if [1/3, 1]\n",
    "'''\n",
    "def classify(predictions, uniform=False):\n",
    "    if uniform :\n",
    "        return predictions.vectorize(lambda x: -1 if x < -1/3 else 0 if x < 1/3 else 1)   \n",
    "    else :\n",
    "        return np.round(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ouwaAkcImG-C"
   },
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "S3LOLQCfgl9n"
   },
   "outputs": [],
   "source": [
    "#Load test sample manually labelled\n",
    "test_sample = pd.read_csv(path_us + \"sample_quotes_labelled.csv\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KE-xfjkDgl9n",
    "outputId": "26312e70-e774-4915-96ba-777816c0dc7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading flair model...\n",
      "2021-12-13 09:24:04,482 https://nlp.informatik.hu-berlin.de/resources/models/sentiment-curated-distilbert/sentiment-en-mix-distillbert_4.pt not found in cache, downloading to /tmp/tmptaz8uy_m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 265512723/265512723 [00:29<00:00, 8975861.41B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-13 09:24:34,903 copying /tmp/tmptaz8uy_m to cache at /root/.flair/models/sentiment-en-mix-distillbert_4.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-13 09:24:35,426 removing temp file /tmp/tmptaz8uy_m\n",
      "2021-12-13 09:24:35,464 loading file /root/.flair/models/sentiment-en-mix-distillbert_4.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08e128e99a784fa4bb7a60299aef93b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5beaa76f2e0049d2a66d3fbde7c3e3fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4f6e76c2fab4e1886beb512b9b908da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26ab86e7ee1545d2b9081756bc6378d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model done in  56.46393418312073 seconds.\n"
     ]
    }
   ],
   "source": [
    "#Loading flair model BERT\n",
    "sia1 = sia_flair_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OSgN-ZukixSc",
    "outputId": "e4f53833-9258-4220-c6a0-cb690eb0fd9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading flair model...\n",
      "2021-12-13 09:25:00,977 https://nlp.informatik.hu-berlin.de/resources/models/sentiment-curated-fasttext-rnn/sentiment-en-mix-ft-rnn_v8.pt not found in cache, downloading to /tmp/tmplbweqtss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1241977025/1241977025 [02:13<00:00, 9281532.97B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-13 09:27:15,664 copying /tmp/tmplbweqtss to cache at /root/.flair/models/sentiment-en-mix-ft-rnn_v8.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-12-13 09:27:20,108 removing temp file /tmp/tmplbweqtss\n",
      "2021-12-13 09:27:20,277 loading file /root/.flair/models/sentiment-en-mix-ft-rnn_v8.pt\n",
      "Loading model done in  150.95562291145325 seconds.\n"
     ]
    }
   ],
   "source": [
    "#Loading flair model RNN\n",
    "sia2 = sia_flair_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0YNyacrvl7D7"
   },
   "source": [
    "### Accuracy comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UMXVxPwsEtwb",
    "outputId": "e2b64514-ecdb-42e0-a958-939434f76327"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flair Analysis\n",
      "===========================\n",
      "mse : 0.4684054682271506\n",
      "accuracy : 0.66 %\n",
      "\n",
      "Flair_fast Analysis\n",
      "===========================\n",
      "mse : 0.5881031352434072\n",
      "accuracy : 0.56 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models_name = [\"Flair\", \"Flair_fast\"]\n",
    "models_sia = [sia1, sia2]\n",
    "models_pred = [pred_flair_1, pred_flair_2]\n",
    "\n",
    "for i in range(len(models_name)):\n",
    "    print(models_name[i] + ' Analysis')\n",
    "    print(\"===========================\")\n",
    "\n",
    "    pred = models_pred[i]\n",
    "    #predict sentiment foreach quotation in the test set\n",
    "    #-1 : negative, 0 : neutral, 1 positive\n",
    "    predictions = pred(test_sample.quotation, models_sia[i], batch_size=16)\n",
    "    #Compute MSE w.r.t. test set\n",
    "    mse(test_sample[\"sentiment\"], predictions)\n",
    "    \n",
    "    sentiments = classify(predictions)\n",
    "    #Compute accuracy w.r.t. test set\n",
    "    accuracy(test_sample[\"sentiment\"], sentiments)\n",
    "\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zt-WpEl_E01A"
   },
   "source": [
    "### Runtime comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dQUbfHYGWcT"
   },
   "source": [
    "Note : running on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bo2sZ8pQgl9p",
    "outputId": "6da5965d-346e-4636-c3d8-e5d39955cc19"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#loading a sample dataset of 10000 quotes\n",
    "df_sample = pd.read_pickle(path_us + \"us_2020.pkl.bz2\",compression='bz2')[['quoteID', 'quotation']].sample(10000)\n",
    "len(df_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dw1BFGFaEhtZ",
    "outputId": "126760bc-725b-4583-eaed-7f0c4d134fb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mini-batch size : 8\n",
      "Runtime flair:  53.888055086135864 seconds \n",
      "Runtime flair_fast:  20.321967124938965 seconds \n",
      "\n",
      "mini-batch size : 16\n",
      "Runtime flair:  48.28710865974426 seconds \n",
      "Runtime flair_fast:  18.333975076675415 seconds \n",
      "\n",
      "mini-batch size : 32\n",
      "Runtime flair:  46.5600368976593 seconds \n",
      "Runtime flair_fast:  15.473497152328491 seconds \n",
      "\n",
      "mini-batch size : 64\n",
      "Runtime flair:  46.0074577331543 seconds \n",
      "Runtime flair_fast:  15.76470422744751 seconds \n",
      "\n",
      "mini-batch size : 128\n",
      "Runtime flair:  49.31395673751831 seconds \n",
      "Runtime flair_fast:  14.79665207862854 seconds \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Comparing the mini-batch size to get the best runtime\n",
    "sizes = [8, 16, 32, 64, 128]\n",
    "for s in sizes: \n",
    "    print(\"mini-batch size : \" + str(s))\n",
    "    #Flair BERT\n",
    "    start_time = time.time()\n",
    "    predictions = pred_flair_1(df_sample.quotation, sia1, batch_size=s)\n",
    "    print(\"Runtime flair:  %s seconds \" % (time.time() - start_time))\n",
    "    #Flair RNN\n",
    "    start_time = time.time()\n",
    "    predictions = pred_flair_2(df_sample.quotation, sia2, batch_size=s)\n",
    "    print(\"Runtime flair_fast:  %s seconds \" % (time.time() - start_time))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMJzw6UiJtDQ"
   },
   "source": [
    "### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8lrxwsHWJwFk"
   },
   "source": [
    "Despite the good accuracy that the first Flair model gives us, its runtime with the computational power we have make its use not practicable. For that reason we have chosen the Flair fast model that runs around 4 times faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "flair_analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
