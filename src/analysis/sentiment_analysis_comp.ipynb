{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Sentiment Analysis Models comparison \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note :  \n",
    "need install tld, nltk, flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import bz2\n",
    "import json\n",
    "import os.path\n",
    "\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/jonathan/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Sentiment analysis libraries\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create sample set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/quotes-2020-domains.json.bz2\n"
     ]
    }
   ],
   "source": [
    "data_path = \"data/quotes-\"\n",
    "name_extension = \"-domains.json.bz2\"\n",
    "path_2020 = data_path + \"2020\" + name_extension\n",
    "print(path_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [] \n",
    "#Read all dataset to extract sample dataset\n",
    "for year in range(2015, 2020 + 1):\n",
    "    file_name = data_path + str(year) + name_extension\n",
    "    data = pd.read_json(file_name, lines=True, compression='bz2') \n",
    "    dfs.append(data) # append the data frame to the list\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True) # concatenate all the data frames in the list.\n",
    "\n",
    "samples = df.sample(n=50, random_state=1)\n",
    "\n",
    "samples['quotation'].to_csv(\"sample_quotes.csv\", index=True)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load labelled quotations\n",
    "test_sample = pd.read_csv(\"sample_quotes_labelled.csv\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "print accuracy given predictions and the test set\n",
    "'''\n",
    "def accuracy(test, output):\n",
    "    res = (test == output).sum()\n",
    "    print(\"accuracy : \" + str(res / len(test)) + \" %\")\n",
    "    return res / len(test)\n",
    "\n",
    "'''\n",
    "print the mean square error given the prediction and the test set\n",
    "'''\n",
    "def mse(test, predictions):\n",
    "    res = ((test - predictions)**2).sum() / len(test)\n",
    "    print('mse : ' + str(res))\n",
    "    return res\n",
    "\n",
    "'''\n",
    "given predictions between [-1, 1], return labels {-1, 0, 1}\n",
    "uniform : if the range is equally splitted : \n",
    "-1 if [-1, -1/3]\n",
    "0 if [-1/3, 1/3]\n",
    "-1 if [1/3, 1]\n",
    "'''\n",
    "def classify(predictions, uniform=False):\n",
    "    if uniform :\n",
    "        return predictions.apply(lambda x: -1 if x < -1/3 else 0 if x < 1/3 else 1)   \n",
    "    else :\n",
    "        return round(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentiment analysis object initialization of each model\n",
    "sia_nlkt = lambda  : SentimentIntensityAnalyzer()\n",
    "sia_blob = lambda : None\n",
    "\n",
    "def sia_flair():\n",
    "    start_time = time.time()\n",
    "    print(\"Loading flair model...\")\n",
    "    sia = TextClassifier.load('sentiment-fast')\n",
    "    print(\"Loading model done in  %s seconds.\" % (time.time() - start_time))\n",
    "    return sia\n",
    "\n",
    "#sentiment analysis prediction of each model : x being the quote\n",
    "pred_nlkt = lambda x, sia: sia.polarity_scores(x)[\"compound\"]\n",
    "pred_blob = lambda x, _: TextBlob(x).sentiment.polarity\n",
    "\n",
    "def pred_flair(x, sia):\n",
    "    sentence = Sentence(x)\n",
    "    sia.predict(sentence,return_probabilities_for_all_classes=True, mini_batch_size=1)\n",
    "    score = -1 * sentence.labels[0].score + sentence.labels[1].score\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Analysis\n",
      "===========================\n",
      "mse : 0.5337384212\n",
      "accuracy : 0.54 %\n",
      "Runtime :  0.02494192123413086 seconds \n",
      "\n",
      "TextBlob Analysis\n",
      "===========================\n",
      "mse : 0.5716744422026094\n",
      "accuracy : 0.42 %\n",
      "Runtime :  0.021116256713867188 seconds \n",
      "\n",
      "Flair Analysis\n",
      "===========================\n",
      "Loading flair model...\n",
      "2021-12-17 21:40:24,166 loading file /Users/jonathan/.flair/models/sentiment-en-mix-ft-rnn_v8.pt\n",
      "Loading model done in  7.427731990814209 seconds.\n",
      "mse : 2.0846241593628605\n",
      "accuracy : 0.16 %\n",
      "Runtime :  0.3076648712158203 seconds \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#List of models to iterate    \n",
    "models_name = [\"NLTK\", \"TextBlob\", \"Flair\"]\n",
    "models_sia = [sia_nlkt, sia_blob, sia_flair]\n",
    "models_pred = [pred_nlkt, pred_blob, pred_flair]\n",
    "\n",
    "for i in range(3):\n",
    "    print(models_name[i] + ' Analysis')\n",
    "    print(\"===========================\")\n",
    "    sia = models_sia[i]()\n",
    "    start_time = time.time()\n",
    "    pred = models_pred[i]\n",
    "    #predict sentiment foreach quotation in the test set\n",
    "    #-1 : negative, 0 : neutral, 1 positive\n",
    "    predictions = test_sample.quotation.apply(lambda x : pred(x, sia))\n",
    "    \n",
    "    mse(test_sample[\"sentiment\"], predictions)\n",
    "    \n",
    "    sentiments = classify(predictions)\n",
    "    accuracy(test_sample[\"sentiment\"], sentiments)\n",
    "    \n",
    "    print(\"Runtime :  %s seconds \" % (time.time() - start_time))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "The runtime comparison was made using CPU. Because the flair model uses neural networks internally, it will have greater performances on GPUs. For further analysis of Flair, please refer to the flairs_analysis.ipynb notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
