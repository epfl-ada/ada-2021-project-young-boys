{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "sentiment_extraction_flair.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Import and installations"
      ],
      "metadata": {
        "id": "qhuBvwa0mMEf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7i81Yw3jgl9T"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNJQIjxAhdEB",
        "outputId": "690a432b-a523-4655-c9f8-64eac58c95d5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flair"
      ],
      "metadata": {
        "id": "JHk7iQDmMCIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rl3Xq_xVg2cC"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45Czlskdgl9Y"
      },
      "source": [
        "import bz2\n",
        "import json\n",
        "import os.path\n",
        "import pandas as pd\n",
        "import time\n",
        "from flair.models import TextClassifier\n",
        "from flair.data import Sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_files = \"/content/drive/MyDrive/\"\n",
        "path_us = path_files + \"us_data/Filtered data/\"\n",
        "path_out = path_files + \"data/\"\n",
        "\n",
        "pref = \"quotes-\"\n",
        "suff = \".json.bz2\"\n",
        "data_names = []\n",
        "data_names_speak = []\n",
        "data_names_speak_sent = []\n",
        "\n",
        "data_names_s = []\n",
        "data_names_speak_s = []\n",
        "data_names_speak_sent_s = []\n",
        "year = {}\n",
        "#Create the names to get the data and to store the data\n",
        "#Note : 2 datasets were created for each year, \n",
        "# for that reason there are the variables data_names* and data_names_speak*\n",
        "for (index, i) in enumerate(range(2015, 2021)):\n",
        "  year[i] = index\n",
        "  s = pref + str(i) + \"-us\"\n",
        "  data_names.append(s + suff)\n",
        "  data_names_speak.append(s + \"_speakers_\" + str(i) + suff)\n",
        "  data_names_speak_sent.append(s + \"_speakers_\" + \"_sent_\" + suff)\n",
        "\n",
        "  data_names_s.append(s  + \"_sample\"+ suff)\n",
        "  data_names_speak_s.append(s + \"_speakers_\" + str(i) + \"_sample\"+  suff)\n",
        "  data_names_speak_sent_s.append(s + \"_speakers_\" + \"_sent_\" + \"_sample\"+ suff)\n",
        "\n",
        "\n",
        "print(data_names)\n",
        "print(data_names_speak)"
      ],
      "metadata": {
        "id": "HJ57mQianJqS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7caafdfc-02bf-4c4d-9859-4959e4ce5bfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['quotes-2015-us.json.bz2', 'quotes-2016-us.json.bz2', 'quotes-2017-us.json.bz2', 'quotes-2018-us.json.bz2', 'quotes-2019-us.json.bz2', 'quotes-2020-us.json.bz2']\n",
            "['quotes-2015-us_speakers_2015.json.bz2', 'quotes-2016-us_speakers_2016.json.bz2', 'quotes-2017-us_speakers_2017.json.bz2', 'quotes-2018-us_speakers_2018.json.bz2', 'quotes-2019-us_speakers_2019.json.bz2', 'quotes-2020-us_speakers_2020.json.bz2']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tools"
      ],
      "metadata": {
        "id": "nktVmqTRl1_R"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xx0s4Y25gl9m"
      },
      "source": [
        "def sia_flair():\n",
        "    start_time = time.time()\n",
        "    print(\"Loading flair model...\")\n",
        "    sia = TextClassifier.load('sentiment-fast')\n",
        "    print(\"Loading model done in  %s seconds.\" % (time.time() - start_time))\n",
        "    return sia\n",
        "\n",
        "def pred_flair(x, sia, batch_size):\n",
        "    x_np = x.to_numpy()\n",
        "    sentences = list(np.vectorize(lambda a : Sentence(a))(x_np))\n",
        "    sia.predict(sentences,return_probabilities_for_all_classes=True, mini_batch_size=batch_size)\n",
        "    scores = [ -1 * sent.labels[1].score + sent.labels[0].score for sent in sentences]\n",
        "\n",
        "    return np.array(scores)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess"
      ],
      "metadata": {
        "id": "ouwaAkcImG-C"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSgN-ZukixSc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2d9de61-e067-492a-d6cb-673cd83fddf7"
      },
      "source": [
        "sia = sia_flair()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading flair model...\n",
            "2021-12-10 15:33:18,944 https://nlp.informatik.hu-berlin.de/resources/models/sentiment-curated-fasttext-rnn/sentiment-en-mix-ft-rnn_v8.pt not found in cache, downloading to /tmp/tmp_8qh1wwc\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1241977025/1241977025 [01:39<00:00, 12476252.13B/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-10 15:34:58,878 copying /tmp/tmp_8qh1wwc to cache at /root/.flair/models/sentiment-en-mix-ft-rnn_v8.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-10 15:35:03,285 removing temp file /tmp/tmp_8qh1wwc\n",
            "2021-12-10 15:35:03,463 loading file /root/.flair/models/sentiment-en-mix-ft-rnn_v8.pt\n",
            "Loading model done in  127.94758987426758 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process"
      ],
      "metadata": {
        "id": "0YNyacrvl7D7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "get the sentiment chunk by chunk given a dataframe reader\n",
        "'''\n",
        "def reader_get_sentiment(df_r):\n",
        "    df_s = []\n",
        "    for chunk in df_r:\n",
        "      df = pd.DataFrame()\n",
        "      df['quoteId'] = chunk.quoteID\n",
        "      df['sentiment'] = pred_flair_2(chunk.quotation, sia, batch_size=32)\n",
        "      df_s.append(df)\n",
        "\n",
        "    return pd.concat(df_s)"
      ],
      "metadata": {
        "id": "Gf7TxaC1wiV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(2015, 2021):\n",
        "  df_reader = pd.read_json(path_us + data_names[year[i]], lines=True, compression='bz2', chunksize=10000)\n",
        "  df_sent = reader_get_sentiment(df_reader)\n",
        "  df_sent.to_pickle(path_us + \"sent_\" + str(i) + \".pkl\")\n",
        "  df_sent = None\n",
        "  df_reader = None\n",
        "\n",
        "  df_reader = pd.read_json(path_us + data_names_speak[year[i]], lines=True, compression='bz2', chunksize=10000)\n",
        "  df_sent = reader_get_sentiment(df_reader)\n",
        "  df_sent.to_pickle(path_us + \"sent_speaker\" + str(i) + \".pkl\")\n",
        "  df_sent = None\n",
        "  df_reader = None\n"
      ],
      "metadata": {
        "id": "OPivrB9ewV-d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}